# -*- coding: utf-8 -*-
"""IR_gmm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wNXXy6fC-OCSdMaic4qUZpk1yt6uC8an
"""

# ---------------------------------------------------------------
# INSTALL DEPENDENCIES
# ---------------------------------------------------------------
!pip install pandas nltk sentence-transformers keybert scikit-learn tensorflow arxiv tenacity

import pandas as pd
import numpy as np
import nltk
import re
import tensorflow as tf
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

nltk.download('stopwords')
nltk.download('wordnet')

# ---------------------------------------------------------------
# ARXIV FETCH WITH RETRIES + BACKOFF
# ---------------------------------------------------------------
import arxiv
from tenacity import retry, stop_after_attempt, wait_exponential

query = "machine learning"
max_results = 100
client = arxiv.Client(delay_seconds=4.0)
search = arxiv.Search(
    query=query,
    max_results=max_results,
    sort_by=arxiv.SortCriterion.SubmittedDate
)

@retry(stop=stop_after_attempt(5), wait=wait_exponential(multiplier=2))
def safe_results(client, search):
    return list(client.results(search))

print("Fetching papers safely...")
papers = safe_results(client, search)
print("Download complete.")

data = []
for result in papers:
    data.append({
        "title": result.title,
        "authors": ", ".join(a.name for a in result.authors),
        "abstract": result.summary,
        "categories": ", ".join(result.categories),
        "published": result.published
    })

df = pd.DataFrame(data)

# ---------------------------------------------------------------
# TEXT CLEANING
# ---------------------------------------------------------------
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words("english"))

def clean_text(text):
    text = text.lower()
    text = re.sub(r"[^\w\s]", "", text)
    tokens = [lemmatizer.lemmatize(w) for w in text.split() if w not in stop_words]
    return " ".join(tokens)

df["clean_abstract"] = df["abstract"].apply(clean_text)

# ---------------------------------------------------------------
# EMBEDDING + PCA
# ---------------------------------------------------------------
from sentence_transformers import SentenceTransformer

model = SentenceTransformer("all-MiniLM-L6-v2")
embeddings = model.encode(df["clean_abstract"].tolist(), batch_size=16)

from sklearn.decomposition import PCA
pca = PCA(n_components=30)
reduced_embeddings = pca.fit_transform(embeddings)

# ---------------------------------------------------------------
# IDEC DEEP CLUSTERING MODEL
# ---------------------------------------------------------------
from tensorflow.keras import layers, Model

input_dim = reduced_embeddings.shape[1]
# Encoder
inp = layers.Input(shape=(input_dim,))
x = layers.Dense(128, activation="relu")(inp)
latent = layers.Dense(32, activation="relu")(x)

# Decoder
y = layers.Dense(128, activation="relu")(latent)
outp = layers.Dense(input_dim, activation="linear")(y)
autoencoder = Model(inp, outp)
encoder = Model(inp, latent)
autoencoder.compile(optimizer="adam", loss="mse")

print("Pretraining autoencoder...")
autoencoder.fit(reduced_embeddings, reduced_embeddings, epochs=10, batch_size=16, verbose=1)

# ---------------------------------------------------------------
# DEC/IDEC CLUSTER LAYER
# ---------------------------------------------------------------
class ClusteringLayer(layers.Layer):
    def __init__(self, n_clusters, **kwargs):
        super().__init__(**kwargs)
        self.n_clusters = n_clusters

    def build(self, input_shape):
        self.centroids = self.add_weight(
            shape=(self.n_clusters, input_shape[-1]),
            initializer="glorot_uniform",
            trainable=True)

    def call(self, inputs):
        # Student t-distribution, as used in DEC paper
        diff = tf.expand_dims(inputs, 1) - self.centroids
        dist = tf.reduce_sum(diff**2, axis=2)
        q = 1.0 / (1.0 + dist)
        q = tf.transpose(tf.transpose(q) / tf.reduce_sum(q, axis=1))
        return q

n_clusters = 10
cluster_layer = ClusteringLayer(n_clusters)(latent)
idec = Model(inp, [outp, cluster_layer])
idec.compile(optimizer="adam", loss=["mse", "kld"], loss_weights=[1.0, 0.1])

# ---------------------------------------------------------------
# CLUSTER CENTER INITIALIZATION VIA GMM
# ---------------------------------------------------------------
from sklearn.mixture import GaussianMixture

latent_data = encoder.predict(reduced_embeddings)
gmm = GaussianMixture(n_components=n_clusters, covariance_type='full', random_state=0)
gmm_labels = gmm.fit_predict(latent_data)
idec.layers[-1].set_weights([gmm.means_])  # Set GMM means as initial cluster centers

# ---------------------------------------------------------------
# IDEC TRAINING
# ---------------------------------------------------------------
max_epochs = 15
batch_size = 16
for epoch in range(max_epochs):
    q = idec.predict(reduced_embeddings, batch_size=batch_size)[1]
    p = (q**2) / q.sum(axis=0)
    p = p / p.sum(axis=1, keepdims=True)
    for i in range(0, len(reduced_embeddings), batch_size):
        x_batch = reduced_embeddings[i:i+batch_size]
        p_batch = p[i:i+batch_size]
        idec.train_on_batch(x_batch, [x_batch, p_batch])

final_q = idec.predict(reduced_embeddings, batch_size=batch_size)[1]
labels = np.argmax(final_q, axis=1)
df["cluster"] = labels

# ---------------------------------------------------------------
# TOPIC EXTRACTION
# ---------------------------------------------------------------
from keybert import KeyBERT

kw = KeyBERT("all-MiniLM-L6-v2")
topics = {}
for cid in df["cluster"].unique():
    text = " ".join(df[df["cluster"] == cid]["clean_abstract"])
    keywords = kw.extract_keywords(text, top_n=2)
    topics[cid] = ", ".join([k[0] for k in keywords])
df["cluster_topic"] = df["cluster"].map(topics)
df.to_csv("arxiv_abstracts_new_new.csv", index=False)

print("Saved arxiv_abstracts_new_new.csv")